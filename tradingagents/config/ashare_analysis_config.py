#!/usr/bin/env python3
"""
Aè‚¡åˆ†æé…ç½®ç®¡ç†

ä¸“é—¨å¤„ç†Aè‚¡åˆ†æå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é…ç½®ï¼ŒåŒ…æ‹¬ï¼š
1. Aè‚¡åˆ†æAgenté…ç½®
2. æ•°æ®æºé…ç½®(Tushare/AkShare)
3. åˆ†ææ·±åº¦å’Œèµ„æºé™åˆ¶é…ç½®
4. ç¼“å­˜å’Œæ€§èƒ½é…ç½®
"""

import os
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from dotenv import load_dotenv

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('ashare_analysis_config')


@dataclass
class AShareAgentConfig:
    """Aè‚¡åˆ†æAgenté…ç½®"""
    agent_name: str  # Agentåç§°
    enabled: bool = True  # æ˜¯å¦å¯ç”¨
    analysis_depth: str = "standard"  # åˆ†ææ·±åº¦: basic/standard/comprehensive
    max_tokens: int = 2000  # æœ€å¤§tokené™åˆ¶
    temperature: float = 0.7  # ç”Ÿæˆæ¸©åº¦
    timeout_seconds: int = 300  # è¶…æ—¶æ—¶é—´
    retry_count: int = 3  # é‡è¯•æ¬¡æ•°
    parallel_execution: bool = True  # æ˜¯å¦æ”¯æŒå¹¶è¡Œæ‰§è¡Œ


@dataclass
class AShareDataSourceConfig:
    """Aè‚¡æ•°æ®æºé…ç½®"""
    source_name: str  # æ•°æ®æºåç§° (tushare/akshare)
    enabled: bool = True  # æ˜¯å¦å¯ç”¨
    api_key: str = ""  # APIå¯†é’¥
    priority: int = 1  # ä¼˜å…ˆçº§ (1-10, æ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜)
    rate_limit_per_second: int = 10  # æ¯ç§’è¯·æ±‚é™åˆ¶
    cache_ttl_hours: int = 24  # ç¼“å­˜TTLå°æ—¶æ•°
    fallback_enabled: bool = True  # æ˜¯å¦æ”¯æŒä½œä¸ºå¤‡ç”¨æ•°æ®æº


@dataclass
class AShareAnalysisConfig:
    """Aè‚¡åˆ†æé…ç½®"""
    # åŸºç¡€é…ç½®
    enabled: bool = True
    default_analysis_depth: str = "standard"  # basic/standard/comprehensive
    max_concurrent_analyses: int = 3  # æœ€å¤§å¹¶å‘åˆ†ææ•°
    
    # Agenté…ç½®
    agents: List[AShareAgentConfig] = None
    
    # æ•°æ®æºé…ç½®
    data_sources: List[AShareDataSourceConfig] = None
    
    # èµ„æºé™åˆ¶é…ç½®
    max_tokens_per_analysis: int = 8000  # æ¯æ¬¡åˆ†ææœ€å¤§tokenæ•°
    max_analysis_time_minutes: int = 10  # æœ€å¤§åˆ†ææ—¶é—´
    cost_limit_per_analysis_cny: float = 5.0  # æ¯æ¬¡åˆ†ææˆæœ¬é™åˆ¶(äººæ°‘å¸)
    
    # ç¼“å­˜é…ç½®
    cache_enabled: bool = True
    cache_redis_enabled: bool = True
    cache_mongodb_enabled: bool = True
    cache_file_enabled: bool = True
    
    # æŠ¥å‘Šé…ç½®
    report_format: str = "markdown"  # markdown/json/html
    include_raw_data: bool = False  # æ˜¯å¦åŒ…å«åŸå§‹æ•°æ®
    pyramid_structure: bool = True  # æ˜¯å¦ä½¿ç”¨é‡‘å­—å¡”ç»“æ„
    
    # è°ƒè¯•é…ç½®
    debug_mode: bool = False
    log_level: str = "INFO"
    save_intermediate_results: bool = False


class AShareAnalysisConfigManager:
    """Aè‚¡åˆ†æé…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: str = "config"):
        self.config_dir = config_dir
        self.config_file = os.path.join(config_dir, "ashare_analysis.json")
        
        # åŠ è½½ç¯å¢ƒå˜é‡
        self._load_env_variables()
        
        # åˆå§‹åŒ–é»˜è®¤é…ç½®
        self._init_default_config()
    
    def _load_env_variables(self):
        """åŠ è½½ç¯å¢ƒå˜é‡"""
        # å°è¯•åŠ è½½.envæ–‡ä»¶
        try:
            load_dotenv()
        except ImportError:
            pass
    
    def _init_default_config(self) -> AShareAnalysisConfig:
        """åˆå§‹åŒ–é»˜è®¤Aè‚¡åˆ†æé…ç½®"""
        
        # é»˜è®¤Agenté…ç½®
        default_agents = [
            AShareAgentConfig(
                agent_name="financial_metrics",
                enabled=True,
                analysis_depth="standard",
                max_tokens=2000,
                temperature=0.7,
                timeout_seconds=300
            ),
            AShareAgentConfig(
                agent_name="industry_comparison", 
                enabled=True,
                analysis_depth="standard",
                max_tokens=2000,
                temperature=0.7,
                timeout_seconds=300
            ),
            AShareAgentConfig(
                agent_name="valuation_analysis",
                enabled=True,
                analysis_depth="standard", 
                max_tokens=2000,
                temperature=0.7,
                timeout_seconds=300
            ),
            AShareAgentConfig(
                agent_name="report_integration",
                enabled=True,
                analysis_depth="comprehensive",
                max_tokens=3000,
                temperature=0.7,
                timeout_seconds=600,
                parallel_execution=False  # æŠ¥å‘Šæ•´åˆä¸èƒ½å¹¶è¡Œ
            )
        ]
        
        # é»˜è®¤æ•°æ®æºé…ç½®
        default_data_sources = [
            AShareDataSourceConfig(
                source_name="tushare",
                enabled=self._parse_bool_env("TUSHARE_ENABLED", False),
                api_key=os.getenv("TUSHARE_TOKEN", ""),
                priority=1,
                rate_limit_per_second=10,
                cache_ttl_hours=24,
                fallback_enabled=True
            ),
            AShareDataSourceConfig(
                source_name="akshare",
                enabled=self._parse_bool_env("AKSHARE_ENABLED", True),
                api_key="",  # AkShareä¸éœ€è¦APIå¯†é’¥
                priority=2,
                rate_limit_per_second=20,
                cache_ttl_hours=12,
                fallback_enabled=True
            )
        ]
        
        # åŸºç¡€é…ç½®
        config = AShareAnalysisConfig(
            enabled=self._parse_bool_env("ASHARE_ANALYSIS_ENABLED", True),
            default_analysis_depth=os.getenv("ASHARE_DEFAULT_DEPTH", "standard"),
            max_concurrent_analyses=int(os.getenv("ASHARE_MAX_CONCURRENT", "3")),
            agents=default_agents,
            data_sources=default_data_sources,
            max_tokens_per_analysis=int(os.getenv("ASHARE_MAX_TOKENS", "8000")),
            max_analysis_time_minutes=int(os.getenv("ASHARE_MAX_TIME_MINUTES", "10")),
            cost_limit_per_analysis_cny=float(os.getenv("ASHARE_COST_LIMIT_CNY", "5.0")),
            cache_enabled=self._parse_bool_env("ASHARE_CACHE_ENABLED", True),
            cache_redis_enabled=self._parse_bool_env("ASHARE_REDIS_CACHE", True),
            cache_mongodb_enabled=self._parse_bool_env("ASHARE_MONGODB_CACHE", True),
            cache_file_enabled=self._parse_bool_env("ASHARE_FILE_CACHE", True),
            report_format=os.getenv("ASHARE_REPORT_FORMAT", "markdown"),
            include_raw_data=self._parse_bool_env("ASHARE_INCLUDE_RAW_DATA", False),
            pyramid_structure=self._parse_bool_env("ASHARE_PYRAMID_STRUCTURE", True),
            debug_mode=self._parse_bool_env("ASHARE_DEBUG_MODE", False),
            log_level=os.getenv("ASHARE_LOG_LEVEL", "INFO"),
            save_intermediate_results=self._parse_bool_env("ASHARE_SAVE_INTERMEDIATE", False)
        )
        
        return config
    
    def _parse_bool_env(self, env_name: str, default: bool) -> bool:
        """è§£æå¸ƒå°”ç¯å¢ƒå˜é‡"""
        value = os.getenv(env_name, "").lower()
        if value in ("true", "1", "yes", "on"):
            return True
        elif value in ("false", "0", "no", "off"):
            return False
        else:
            return default
    
    def load_config(self) -> AShareAnalysisConfig:
        """åŠ è½½Aè‚¡åˆ†æé…ç½®"""
        try:
            # é¦–å…ˆè·å–é»˜è®¤é…ç½®
            config = self._init_default_config()
            
            # å¦‚æœå­˜åœ¨é…ç½®æ–‡ä»¶ï¼Œåˆ™åˆå¹¶é…ç½®
            if os.path.exists(self.config_file):
                import json
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    file_config = json.load(f)
                    # è¿™é‡Œå¯ä»¥æ·»åŠ é…ç½®åˆå¹¶é€»è¾‘
                    logger.info(f"âœ… åŠ è½½Aè‚¡åˆ†æé…ç½®æ–‡ä»¶: {self.config_file}")
            
            # éªŒè¯é…ç½®
            self._validate_config(config)
            
            logger.info(f"âœ… Aè‚¡åˆ†æé…ç½®åŠ è½½å®Œæˆ")
            return config
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½Aè‚¡åˆ†æé…ç½®å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤é…ç½®
            return self._init_default_config()
    
    def save_config(self, config: AShareAnalysisConfig):
        """ä¿å­˜Aè‚¡åˆ†æé…ç½®"""
        try:
            import json
            
            # ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨
            os.makedirs(self.config_dir, exist_ok=True)
            
            # è½¬æ¢ä¸ºå­—å…¸
            config_dict = asdict(config)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config_dict, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Aè‚¡åˆ†æé…ç½®å·²ä¿å­˜: {self.config_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜Aè‚¡åˆ†æé…ç½®å¤±è´¥: {e}")
    
    def _validate_config(self, config: AShareAnalysisConfig):
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        issues = []
        
        # æ£€æŸ¥åŸºç¡€é…ç½®
        if config.max_concurrent_analyses <= 0:
            issues.append("max_concurrent_analyses å¿…é¡»å¤§äº0")
        
        if config.max_tokens_per_analysis <= 0:
            issues.append("max_tokens_per_analysis å¿…é¡»å¤§äº0")
        
        if config.cost_limit_per_analysis_cny <= 0:
            issues.append("cost_limit_per_analysis_cny å¿…é¡»å¤§äº0")
        
        # æ£€æŸ¥Agenté…ç½®
        if not config.agents:
            issues.append("è‡³å°‘éœ€è¦é…ç½®ä¸€ä¸ªAgent")
        else:
            enabled_agents = [agent for agent in config.agents if agent.enabled]
            if not enabled_agents:
                issues.append("è‡³å°‘éœ€è¦å¯ç”¨ä¸€ä¸ªAgent")
        
        # æ£€æŸ¥æ•°æ®æºé…ç½®
        if not config.data_sources:
            issues.append("è‡³å°‘éœ€è¦é…ç½®ä¸€ä¸ªæ•°æ®æº")
        else:
            enabled_sources = [source for source in config.data_sources if source.enabled]
            if not enabled_sources:
                issues.append("è‡³å°‘éœ€è¦å¯ç”¨ä¸€ä¸ªæ•°æ®æº")
        
        # æ£€æŸ¥åˆ†ææ·±åº¦è®¾ç½®
        valid_depths = ["basic", "standard", "comprehensive"]
        if config.default_analysis_depth not in valid_depths:
            issues.append(f"default_analysis_depth å¿…é¡»æ˜¯ {valid_depths} ä¹‹ä¸€")
        
        # æŠ¥å‘ŠéªŒè¯ç»“æœ
        if issues:
            logger.warning(f"âš ï¸ Aè‚¡åˆ†æé…ç½®éªŒè¯å‘ç°é—®é¢˜:")
            for issue in issues:
                logger.warning(f"   - {issue}")
        else:
            logger.info(f"âœ… Aè‚¡åˆ†æé…ç½®éªŒè¯é€šè¿‡")
    
    def get_enabled_agents(self, config: AShareAnalysisConfig) -> List[AShareAgentConfig]:
        """è·å–å¯ç”¨çš„Agenté…ç½®"""
        return [agent for agent in config.agents if agent.enabled]
    
    def get_enabled_data_sources(self, config: AShareAnalysisConfig) -> List[AShareDataSourceConfig]:
        """è·å–å¯ç”¨çš„æ•°æ®æºé…ç½®"""
        sources = [source for source in config.data_sources if source.enabled]
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        return sorted(sources, key=lambda x: x.priority)
    
    def get_primary_data_source(self, config: AShareAnalysisConfig) -> Optional[AShareDataSourceConfig]:
        """è·å–ä¸»è¦æ•°æ®æº"""
        enabled_sources = self.get_enabled_data_sources(config)
        if enabled_sources:
            return enabled_sources[0]  # ä¼˜å…ˆçº§æœ€é«˜çš„
        return None
    
    def get_fallback_data_sources(self, config: AShareAnalysisConfig) -> List[AShareDataSourceConfig]:
        """è·å–å¤‡ç”¨æ•°æ®æº"""
        enabled_sources = self.get_enabled_data_sources(config)
        return [source for source in enabled_sources[1:] if source.fallback_enabled]
    
    def get_agent_config(self, config: AShareAnalysisConfig, agent_name: str) -> Optional[AShareAgentConfig]:
        """æ ¹æ®åç§°è·å–Agenté…ç½®"""
        for agent in config.agents:
            if agent.agent_name == agent_name and agent.enabled:
                return agent
        return None
    
    def calculate_estimated_cost(self, config: AShareAnalysisConfig, estimated_tokens: int) -> float:
        """è®¡ç®—é¢„ä¼°æˆæœ¬(åŸºäºé…ç½®çš„æˆæœ¬é™åˆ¶)"""
        if estimated_tokens <= 0:
            return 0.0
        
        # åŸºäºtokenæ•°é‡å’Œé…ç½®çš„æˆæœ¬é™åˆ¶è¿›è¡Œä¼°ç®—
        token_ratio = estimated_tokens / config.max_tokens_per_analysis
        estimated_cost = config.cost_limit_per_analysis_cny * token_ratio
        
        return min(estimated_cost, config.cost_limit_per_analysis_cny)
    
    def is_cost_within_limit(self, config: AShareAnalysisConfig, estimated_cost: float) -> bool:
        """æ£€æŸ¥æˆæœ¬æ˜¯å¦åœ¨é™åˆ¶èŒƒå›´å†…"""
        return estimated_cost <= config.cost_limit_per_analysis_cny
    
    def get_cache_config(self, config: AShareAnalysisConfig) -> Dict[str, Any]:
        """è·å–ç¼“å­˜é…ç½®"""
        return {
            "enabled": config.cache_enabled,
            "redis_enabled": config.cache_redis_enabled,
            "mongodb_enabled": config.cache_mongodb_enabled,
            "file_enabled": config.cache_file_enabled,
            "default_ttl_hours": 24 if config.data_sources else 12
        }
    
    def get_env_config_summary(self) -> Dict[str, Any]:
        """è·å–ç¯å¢ƒå˜é‡é…ç½®æ‘˜è¦"""
        return {
            "ashare_analysis_enabled": os.getenv("ASHARE_ANALYSIS_ENABLED", "true"),
            "tushare_enabled": os.getenv("TUSHARE_ENABLED", "false"),
            "tushare_token_set": bool(os.getenv("TUSHARE_TOKEN")),
            "akshare_enabled": os.getenv("AKSHARE_ENABLED", "true"),
            "default_analysis_depth": os.getenv("ASHARE_DEFAULT_DEPTH", "standard"),
            "max_concurrent_analyses": os.getenv("ASHARE_MAX_CONCURRENT", "3"),
            "cost_limit_cny": os.getenv("ASHARE_COST_LIMIT_CNY", "5.0"),
            "debug_mode": os.getenv("ASHARE_DEBUG_MODE", "false")
        }


# å…¨å±€é…ç½®ç®¡ç†å™¨å®ä¾‹
ashare_config_manager = AShareAnalysisConfigManager()


def get_ashare_analysis_config() -> AShareAnalysisConfig:
    """è·å–Aè‚¡åˆ†æé…ç½®"""
    return ashare_config_manager.load_config()


def validate_ashare_config() -> Dict[str, Any]:
    """éªŒè¯Aè‚¡åˆ†æé…ç½®"""
    config = get_ashare_analysis_config()
    enabled_agents = ashare_config_manager.get_enabled_agents(config)
    enabled_sources = ashare_config_manager.get_enabled_data_sources(config)
    primary_source = ashare_config_manager.get_primary_data_source(config)
    
    return {
        "config_valid": len(enabled_agents) > 0 and len(enabled_sources) > 0,
        "enabled_agents_count": len(enabled_agents),
        "enabled_agents": [agent.agent_name for agent in enabled_agents],
        "enabled_sources_count": len(enabled_sources),
        "enabled_sources": [source.source_name for source in enabled_sources],
        "primary_data_source": primary_source.source_name if primary_source else None,
        "estimated_cost_per_analysis": config.cost_limit_per_analysis_cny,
        "max_concurrent_analyses": config.max_concurrent_analyses,
        "cache_enabled": config.cache_enabled,
        "debug_mode": config.debug_mode,
        "env_summary": ashare_config_manager.get_env_config_summary()
    }


def diagnose_ashare_config():
    """è¯Šæ–­Aè‚¡åˆ†æé…ç½®é—®é¢˜"""
    print("ğŸ” Aè‚¡åˆ†æé…ç½®è¯Šæ–­")
    print("=" * 60)
    
    validation = validate_ashare_config()
    
    # æ˜¾ç¤ºé…ç½®çŠ¶æ€
    print(f"\nğŸ“Š é…ç½®çŠ¶æ€:")
    print(f"   é…ç½®æœ‰æ•ˆ: {'âœ…' if validation['config_valid'] else 'âŒ'}")
    print(f"   å¯ç”¨Agentæ•°: {validation['enabled_agents_count']}")
    print(f"   å¯ç”¨æ•°æ®æºæ•°: {validation['enabled_sources_count']}")
    print(f"   ä¸»è¦æ•°æ®æº: {validation['primary_data_source'] or 'æœªé…ç½®'}")
    print(f"   é¢„ä¼°æˆæœ¬é™åˆ¶: Â¥{validation['estimated_cost_per_analysis']}")
    print(f"   æœ€å¤§å¹¶å‘æ•°: {validation['max_concurrent_analyses']}")
    
    # æ˜¾ç¤ºAgentè¯¦æƒ…
    print(f"\nğŸ¤– å¯ç”¨çš„Agent:")
    for agent_name in validation['enabled_agents']:
        print(f"   âœ… {agent_name}")
    
    # æ˜¾ç¤ºæ•°æ®æºè¯¦æƒ…
    print(f"\nğŸ“Š å¯ç”¨çš„æ•°æ®æº:")
    for source_name in validation['enabled_sources']:
        print(f"   âœ… {source_name}")
    
    # æ˜¾ç¤ºç¯å¢ƒå˜é‡
    print(f"\nğŸ” ç¯å¢ƒå˜é‡é…ç½®:")
    env_summary = validation['env_summary']
    for key, value in env_summary.items():
        status = "âœ…" if value not in ["false", False, "", None] else "âŒ"
        print(f"   {key}: {status} {value}")
    
    # å»ºè®®
    if not validation['config_valid']:
        print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
        if validation['enabled_agents_count'] == 0:
            print(f"   - è‡³å°‘å¯ç”¨ä¸€ä¸ªåˆ†æAgent")
        if validation['enabled_sources_count'] == 0:
            print(f"   - è‡³å°‘å¯ç”¨ä¸€ä¸ªæ•°æ®æº(æ¨èå¯ç”¨AkShare)")
        if not validation['primary_data_source']:
            print(f"   - é…ç½®æœ‰æ•ˆçš„ä¸»è¦æ•°æ®æº")


if __name__ == "__main__":
    diagnose_ashare_config()